---
title: "Имплементация линейной регрессии на Jax"
author: "Александр Бызов"
date: "2025-04-05"
categories: [jax, linear-regression]
comments:
  utterances:
    repo: AlexandrByzov/jax_dl
lang: ru
---

# Упражнения

## Базовая линейная регрессия

__Задание:__ Напиши простую функцию, которая делает предсказание на основе линейной модели ($y = w*x + b$).

__Решение:__ Для начала сделаем функцию, которая предсказывает `y` на основе `w`, `x` и `b`, и протестируем ее. Для этого воспользуемся модулем [`jax.numpy`](https://docs.jax.dev/en/latest/jax.numpy.html).

```{python}
import jax
import jax.numpy as jnp

def predict_fn(params: tuple[jax.Array, jax.Array], x: jax.Array) -> jax.Array:
  w, b = params
  return jnp.dot(x, w.T) + b
```


Когда мы пишем функции для Jax, то стоит думать про следующие вопросы:

- Нужно ли и получится ли компилировать функцию с `jax.jit()`?
- Нужно ли и получится ли векторизовать функцию с `jax.vmap()`?
- Нужно ли и получится ли распараллелить функцию с `jax.pmap()`?
- Нужно ли и получится ли получить градиент из этой функции с `jax.grad()` или с `jax.value_and_grad()`?

Несмотря на то, что предсказания можно сделать с помощью матричного умножения, я бы хотел попробовать написать эту функцию сразу таким образом, чтобы получится ее скомпилировать, распаралеллить на батчи `jax.vmap()` и распараллелить эти батчи на несколько устройств с помощью `jax.pmap()`. 

API Jax отличается от классического NumPy. Например, массивы Jax неизменяемы по сравнению с NumPy. Каждый раз, когда

__Задание:__ Напиши функцию, которая считает ошибки (loss function). Используй `jax.grad` для того, чтобы получить функцию, которая считает градиент потери для `w` и `b`.