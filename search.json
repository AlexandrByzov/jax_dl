[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Описание",
    "section": "",
    "text": "Этот сайт посвящен моим попыткам изучить Jax для глубинного обучения. В первую очередь я делаю этот сайт для себя, поэтому может выйти очень так себе. Если есть вопросы / пожелания, то можно подать issue или pull request. Не стоит искать здесь лучших практик, потому что на начало блога я буквально прочитал документацию по Jax, и все."
  },
  {
    "objectID": "posts/002_linear_regression_with_jax/index.html",
    "href": "posts/002_linear_regression_with_jax/index.html",
    "title": "Имплементация линейной регрессии на Jax",
    "section": "",
    "text": "Задание: Напиши простую функцию, которая делает предсказание на основе линейной модели (\\(y = w*x + b\\)).\nРешение: Для начала сделаем функцию, которая предсказывает y на основе w, x и b, и протестируем ее. Для этого воспользуемся модулем jax.numpy.\n\nimport jax\nimport jax.numpy as jnp\n\ndef predict_fn(params: tuple[jax.Array, jax.Array], x: jax.Array) -&gt; jax.Array:\n  w, b = params\n  return jnp.dot(x, w.T) + b\n\nКогда мы пишем функции для Jax, то стоит думать про следующие вопросы:\n\nНужно ли и получится ли компилировать функцию с jax.jit()?\nНужно ли и получится ли векторизовать функцию с jax.vmap()?\nНужно ли и получится ли распараллелить функцию с jax.pmap()?\nНужно ли и получится ли получить градиент из этой функции с jax.grad() или с jax.value_and_grad()?\n\nНесмотря на то, что предсказания можно сделать с помощью матричного умножения, я бы хотел попробовать написать эту функцию сразу таким образом, чтобы получится ее скомпилировать, распаралеллить на батчи jax.vmap() и распараллелить эти батчи на несколько устройств с помощью jax.pmap().\nAPI Jax отличается от классического NumPy. Например, массивы Jax неизменяемы по сравнению с NumPy. Каждый раз, когда\nЗадание: Напиши функцию, которая считает ошибки (loss function). Используй jax.grad для того, чтобы получить функцию, которая считает градиент потери для w и b."
  },
  {
    "objectID": "posts/002_linear_regression_with_jax/index.html#базовая-линейная-регрессия",
    "href": "posts/002_linear_regression_with_jax/index.html#базовая-линейная-регрессия",
    "title": "Имплементация линейной регрессии на Jax",
    "section": "",
    "text": "Задание: Напиши простую функцию, которая делает предсказание на основе линейной модели (\\(y = w*x + b\\)).\nРешение: Для начала сделаем функцию, которая предсказывает y на основе w, x и b, и протестируем ее. Для этого воспользуемся модулем jax.numpy.\n\nimport jax\nimport jax.numpy as jnp\n\ndef predict_fn(params: tuple[jax.Array, jax.Array], x: jax.Array) -&gt; jax.Array:\n  w, b = params\n  return jnp.dot(x, w.T) + b\n\nКогда мы пишем функции для Jax, то стоит думать про следующие вопросы:\n\nНужно ли и получится ли компилировать функцию с jax.jit()?\nНужно ли и получится ли векторизовать функцию с jax.vmap()?\nНужно ли и получится ли распараллелить функцию с jax.pmap()?\nНужно ли и получится ли получить градиент из этой функции с jax.grad() или с jax.value_and_grad()?\n\nНесмотря на то, что предсказания можно сделать с помощью матричного умножения, я бы хотел попробовать написать эту функцию сразу таким образом, чтобы получится ее скомпилировать, распаралеллить на батчи jax.vmap() и распараллелить эти батчи на несколько устройств с помощью jax.pmap().\nAPI Jax отличается от классического NumPy. Например, массивы Jax неизменяемы по сравнению с NumPy. Каждый раз, когда\nЗадание: Напиши функцию, которая считает ошибки (loss function). Используй jax.grad для того, чтобы получить функцию, которая считает градиент потери для w и b."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Заметки",
    "section": "",
    "text": "Имплементация линейной регрессии на Jax\n\n\n\n\n\n\n\n\n\n\n\n5 апр. 2025 г.\n\n\nАлександр Бызов\n\n\n\n\n\n\nНет подходящих элементов"
  }
]